{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"JDK 8\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count\n",
    "# from pyspark.sql.types import IntegerType\n",
    "\n",
    "# from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Saving and Project\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(1, \"Moby Dick\", \"Herman Melville\", 1851),\n",
    "        (2, \"Treasure Island\", \"Robert Louis Stevenson\", 1883),\n",
    "        (3, \"Robinson Crusoe\", \"Daniel Defoe\", 1719),\n",
    "        (4, \"The Fellowship of the Ring\", \"J.R.R. Tolkien\", 1954),\n",
    "        (5, \"The Hitchhiker's Guide to the Galaxy\", \"Douglas Adams\", 1979)]\n",
    "\n",
    "columns = [\"ID\", \"book_name\", \"author\", \"publish_date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+------------+\n",
      "| ID|           book_name|              author|publish_date|\n",
      "+---+--------------------+--------------------+------------+\n",
      "|  1|           Moby Dick|     Herman Melville|        1851|\n",
      "|  2|     Treasure Island|Robert Louis Stev...|        1883|\n",
      "|  3|     Robinson Crusoe|        Daniel Defoe|        1719|\n",
      "|  4|The Fellowship of...|      J.R.R. Tolkien|        1954|\n",
      "|  5|The Hitchhiker's ...|       Douglas Adams|        1979|\n",
      "+---+--------------------+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_books = spark.createDataFrame(data, columns)\n",
    "df_books.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/01 10:21:12 ERROR Executor: Exception in task 2.0 in stage 4.0 (TID 14) 4]\n",
      "java.lang.InternalError: java.io.FileNotFoundException: /home/yuri/Documents/Data Engineering/PySpark/JDK 8/jre/lib/ext/cldrdata.jar\n",
      "\tat sun.misc.URLClassPath$JarLoader.getResource(URLClassPath.java:1066)\n",
      "\tat sun.misc.URLClassPath.getResource(URLClassPath.java:250)\n",
      "\tat java.net.URLClassLoader$1.run(URLClassLoader.java:366)\n",
      "\tat java.net.URLClassLoader$1.run(URLClassLoader.java:363)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat java.net.URLClassLoader.findClass(URLClassLoader.java:362)\n",
      "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n",
      "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n",
      "\tat java.util.ResourceBundle$RBClassLoader.loadClass(ResourceBundle.java:512)\n",
      "\tat java.util.ResourceBundle$Control.newBundle(ResourceBundle.java:2657)\n",
      "\tat java.util.ResourceBundle.loadBundle(ResourceBundle.java:1518)\n",
      "\tat java.util.ResourceBundle.findBundle(ResourceBundle.java:1482)\n",
      "\tat java.util.ResourceBundle.getBundleImpl(ResourceBundle.java:1370)\n",
      "\tat java.util.ResourceBundle.getBundle(ResourceBundle.java:899)\n",
      "\tat sun.util.resources.LocaleData$2.run(LocaleData.java:178)\n",
      "\tat sun.util.resources.LocaleData$2.run(LocaleData.java:173)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat sun.util.resources.LocaleData.getSupplementary(LocaleData.java:173)\n",
      "\tat sun.util.resources.LocaleData.setSupplementary(LocaleData.java:143)\n",
      "\tat sun.util.resources.LocaleData.setSupplementary(LocaleData.java:133)\n",
      "\tat sun.util.locale.provider.LocaleResources.getJavaTimeFormatData(LocaleResources.java:457)\n",
      "\tat sun.util.locale.provider.LocaleResources.getJavaTimeNames(LocaleResources.java:344)\n",
      "\tat sun.util.locale.provider.CalendarNameProviderImpl.getDisplayNamesImpl(CalendarNameProviderImpl.java:157)\n",
      "\tat sun.util.locale.provider.CalendarNameProviderImpl.getJavaTimeDisplayNames(CalendarNameProviderImpl.java:147)\n",
      "\tat sun.util.locale.provider.CalendarDataUtility$CalendarFieldValueNamesMapGetter.getObject(CalendarDataUtility.java:175)\n",
      "\tat sun.util.locale.provider.CalendarDataUtility$CalendarFieldValueNamesMapGetter.getObject(CalendarDataUtility.java:154)\n",
      "\tat sun.util.locale.provider.LocaleServiceProviderPool.getLocalizedObjectImpl(LocaleServiceProviderPool.java:281)\n",
      "\tat sun.util.locale.provider.LocaleServiceProviderPool.getLocalizedObject(LocaleServiceProviderPool.java:265)\n",
      "\tat sun.util.locale.provider.CalendarDataUtility.retrieveJavaTimeFieldValueNames(CalendarDataUtility.java:96)\n",
      "\tat java.time.format.DateTimeTextProvider.createStore(DateTimeTextProvider.java:352)\n",
      "\tat java.time.format.DateTimeTextProvider.findStore(DateTimeTextProvider.java:312)\n",
      "\tat java.time.format.DateTimeTextProvider.getText(DateTimeTextProvider.java:141)\n",
      "\tat java.time.format.DateTimeFormatterBuilder$TextPrinterParser.format(DateTimeFormatterBuilder.java:3110)\n",
      "\tat java.time.format.DateTimeFormatterBuilder$CompositePrinterParser.format(DateTimeFormatterBuilder.java:2190)\n",
      "\tat java.time.format.DateTimeFormatter.formatTo(DateTimeFormatter.java:1746)\n",
      "\tat java.time.format.DateTimeFormatter.format(DateTimeFormatter.java:1720)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.<init>(DateTimeFormatterHelper.scala:272)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.<clinit>(DateTimeFormatterHelper.scala)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper.getOrCreateFormatter(DateTimeFormatterHelper.scala:120)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper.getOrCreateFormatter$(DateTimeFormatterHelper.scala:116)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.getOrCreateFormatter(TimestampFormatter.scala:153)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.formatter$lzycompute(TimestampFormatter.scala:162)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.formatter(TimestampFormatter.scala:161)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.validatePatternString(TimestampFormatter.scala:254)\n",
      "\tat org.apache.spark.sql.catalyst.util.TimestampFormatter$.getFormatter(TimestampFormatter.scala:523)\n",
      "\tat org.apache.spark.sql.catalyst.util.TimestampFormatter$.apply(TimestampFormatter.scala:557)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityGenerator.<init>(UnivocityGenerator.scala:51)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:40)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.io.FileNotFoundException: /home/yuri/Documents/Data Engineering/PySpark/JDK 8/jre/lib/ext/cldrdata.jar\n",
      "\tat sun.misc.URLClassPath$JarLoader.getJarFile(URLClassPath.java:939)\n",
      "\tat sun.misc.URLClassPath$JarLoader.access$800(URLClassPath.java:802)\n",
      "\tat sun.misc.URLClassPath$JarLoader$1.run(URLClassPath.java:887)\n",
      "\tat sun.misc.URLClassPath$JarLoader$1.run(URLClassPath.java:880)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat sun.misc.URLClassPath$JarLoader.ensureOpen(URLClassPath.java:879)\n",
      "\tat sun.misc.URLClassPath$JarLoader.getResource(URLClassPath.java:1064)\n",
      "\t... 68 more\n",
      "25/02/01 10:21:12 ERROR Executor: Exception in task 0.0 in stage 4.0 (TID 12)\n",
      "java.lang.NoClassDefFoundError: Could not initialize class org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper.getOrCreateFormatter(DateTimeFormatterHelper.scala:120)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper.getOrCreateFormatter$(DateTimeFormatterHelper.scala:116)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.getOrCreateFormatter(TimestampFormatter.scala:153)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.formatter$lzycompute(TimestampFormatter.scala:162)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.formatter(TimestampFormatter.scala:161)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.validatePatternString(TimestampFormatter.scala:254)\n",
      "\tat org.apache.spark.sql.catalyst.util.TimestampFormatter$.getFormatter(TimestampFormatter.scala:523)\n",
      "\tat org.apache.spark.sql.catalyst.util.TimestampFormatter$.apply(TimestampFormatter.scala:557)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityGenerator.<init>(UnivocityGenerator.scala:51)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:40)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "25/02/01 10:21:12 ERROR Executor: Exception in task 3.0 in stage 4.0 (TID 15)\n",
      "java.lang.NoClassDefFoundError: Could not initialize class org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper.getOrCreateFormatter(DateTimeFormatterHelper.scala:120)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper.getOrCreateFormatter$(DateTimeFormatterHelper.scala:116)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.getOrCreateFormatter(TimestampFormatter.scala:153)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.formatter$lzycompute(TimestampFormatter.scala:162)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.formatter(TimestampFormatter.scala:161)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.validatePatternString(TimestampFormatter.scala:254)\n",
      "\tat org.apache.spark.sql.catalyst.util.TimestampFormatter$.getFormatter(TimestampFormatter.scala:523)\n",
      "\tat org.apache.spark.sql.catalyst.util.TimestampFormatter$.apply(TimestampFormatter.scala:557)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityGenerator.<init>(UnivocityGenerator.scala:51)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:40)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "25/02/01 10:21:12 ERROR Executor: Exception in task 1.0 in stage 4.0 (TID 13)\n",
      "java.lang.NoClassDefFoundError: Could not initialize class org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper.getOrCreateFormatter(DateTimeFormatterHelper.scala:120)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper.getOrCreateFormatter$(DateTimeFormatterHelper.scala:116)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.getOrCreateFormatter(TimestampFormatter.scala:153)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.formatter$lzycompute(TimestampFormatter.scala:162)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.formatter(TimestampFormatter.scala:161)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.validatePatternString(TimestampFormatter.scala:254)\n",
      "\tat org.apache.spark.sql.catalyst.util.TimestampFormatter$.getFormatter(TimestampFormatter.scala:523)\n",
      "\tat org.apache.spark.sql.catalyst.util.TimestampFormatter$.apply(TimestampFormatter.scala:557)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityGenerator.<init>(UnivocityGenerator.scala:51)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:40)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "25/02/01 10:21:12 WARN TaskSetManager: Lost task 1.0 in stage 4.0 (TID 13) (192.168.1.249 executor driver): java.lang.NoClassDefFoundError: Could not initialize class org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper.getOrCreateFormatter(DateTimeFormatterHelper.scala:120)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper.getOrCreateFormatter$(DateTimeFormatterHelper.scala:116)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.getOrCreateFormatter(TimestampFormatter.scala:153)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.formatter$lzycompute(TimestampFormatter.scala:162)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.formatter(TimestampFormatter.scala:161)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.validatePatternString(TimestampFormatter.scala:254)\n",
      "\tat org.apache.spark.sql.catalyst.util.TimestampFormatter$.getFormatter(TimestampFormatter.scala:523)\n",
      "\tat org.apache.spark.sql.catalyst.util.TimestampFormatter$.apply(TimestampFormatter.scala:557)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityGenerator.<init>(UnivocityGenerator.scala:51)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:40)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "25/02/01 10:21:12 ERROR TaskSetManager: Task 1 in stage 4.0 failed 1 times; aborting job\n",
      "25/02/01 10:21:12 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 2.0 in stage 4.0 (TID 14),5,main]\n",
      "java.lang.InternalError: java.io.FileNotFoundException: /home/yuri/Documents/Data Engineering/PySpark/JDK 8/jre/lib/ext/cldrdata.jar\n",
      "\tat sun.misc.URLClassPath$JarLoader.getResource(URLClassPath.java:1066)\n",
      "\tat sun.misc.URLClassPath.getResource(URLClassPath.java:250)\n",
      "\tat java.net.URLClassLoader$1.run(URLClassLoader.java:366)\n",
      "\tat java.net.URLClassLoader$1.run(URLClassLoader.java:363)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat java.net.URLClassLoader.findClass(URLClassLoader.java:362)\n",
      "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n",
      "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n",
      "\tat java.util.ResourceBundle$RBClassLoader.loadClass(ResourceBundle.java:512)\n",
      "\tat java.util.ResourceBundle$Control.newBundle(ResourceBundle.java:2657)\n",
      "\tat java.util.ResourceBundle.loadBundle(ResourceBundle.java:1518)\n",
      "\tat java.util.ResourceBundle.findBundle(ResourceBundle.java:1482)\n",
      "\tat java.util.ResourceBundle.getBundleImpl(ResourceBundle.java:1370)\n",
      "\tat java.util.ResourceBundle.getBundle(ResourceBundle.java:899)\n",
      "\tat sun.util.resources.LocaleData$2.run(LocaleData.java:178)\n",
      "\tat sun.util.resources.LocaleData$2.run(LocaleData.java:173)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat sun.util.resources.LocaleData.getSupplementary(LocaleData.java:173)\n",
      "\tat sun.util.resources.LocaleData.setSupplementary(LocaleData.java:143)\n",
      "\tat sun.util.resources.LocaleData.setSupplementary(LocaleData.java:133)\n",
      "\tat sun.util.locale.provider.LocaleResources.getJavaTimeFormatData(LocaleResources.java:457)\n",
      "\tat sun.util.locale.provider.LocaleResources.getJavaTimeNames(LocaleResources.java:344)\n",
      "\tat sun.util.locale.provider.CalendarNameProviderImpl.getDisplayNamesImpl(CalendarNameProviderImpl.java:157)\n",
      "\tat sun.util.locale.provider.CalendarNameProviderImpl.getJavaTimeDisplayNames(CalendarNameProviderImpl.java:147)\n",
      "\tat sun.util.locale.provider.CalendarDataUtility$CalendarFieldValueNamesMapGetter.getObject(CalendarDataUtility.java:175)\n",
      "\tat sun.util.locale.provider.CalendarDataUtility$CalendarFieldValueNamesMapGetter.getObject(CalendarDataUtility.java:154)\n",
      "\tat sun.util.locale.provider.LocaleServiceProviderPool.getLocalizedObjectImpl(LocaleServiceProviderPool.java:281)\n",
      "\tat sun.util.locale.provider.LocaleServiceProviderPool.getLocalizedObject(LocaleServiceProviderPool.java:265)\n",
      "\tat sun.util.locale.provider.CalendarDataUtility.retrieveJavaTimeFieldValueNames(CalendarDataUtility.java:96)\n",
      "\tat java.time.format.DateTimeTextProvider.createStore(DateTimeTextProvider.java:352)\n",
      "\tat java.time.format.DateTimeTextProvider.findStore(DateTimeTextProvider.java:312)\n",
      "\tat java.time.format.DateTimeTextProvider.getText(DateTimeTextProvider.java:141)\n",
      "\tat java.time.format.DateTimeFormatterBuilder$TextPrinterParser.format(DateTimeFormatterBuilder.java:3110)\n",
      "\tat java.time.format.DateTimeFormatterBuilder$CompositePrinterParser.format(DateTimeFormatterBuilder.java:2190)\n",
      "\tat java.time.format.DateTimeFormatter.formatTo(DateTimeFormatter.java:1746)\n",
      "\tat java.time.format.DateTimeFormatter.format(DateTimeFormatter.java:1720)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.<init>(DateTimeFormatterHelper.scala:272)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.<clinit>(DateTimeFormatterHelper.scala)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper.getOrCreateFormatter(DateTimeFormatterHelper.scala:120)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper.getOrCreateFormatter$(DateTimeFormatterHelper.scala:116)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.getOrCreateFormatter(TimestampFormatter.scala:153)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.formatter$lzycompute(TimestampFormatter.scala:162)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.formatter(TimestampFormatter.scala:161)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.validatePatternString(TimestampFormatter.scala:254)\n",
      "\tat org.apache.spark.sql.catalyst.util.TimestampFormatter$.getFormatter(TimestampFormatter.scala:523)\n",
      "\tat org.apache.spark.sql.catalyst.util.TimestampFormatter$.apply(TimestampFormatter.scala:557)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityGenerator.<init>(UnivocityGenerator.scala:51)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:40)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.io.FileNotFoundException: /home/yuri/Documents/Data Engineering/PySpark/JDK 8/jre/lib/ext/cldrdata.jar\n",
      "\tat sun.misc.URLClassPath$JarLoader.getJarFile(URLClassPath.java:939)\n",
      "\tat sun.misc.URLClassPath$JarLoader.access$800(URLClassPath.java:802)\n",
      "\tat sun.misc.URLClassPath$JarLoader$1.run(URLClassPath.java:887)\n",
      "\tat sun.misc.URLClassPath$JarLoader$1.run(URLClassPath.java:880)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat sun.misc.URLClassPath$JarLoader.ensureOpen(URLClassPath.java:879)\n",
      "\tat sun.misc.URLClassPath$JarLoader.getResource(URLClassPath.java:1064)\n",
      "\t... 68 more\n",
      "25/02/01 10:21:12 ERROR FileFormatWriter: Aborting job 73d8a507-2ff3-4db9-b77b-9af943383eaa.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 4.0 failed 1 times, most recent failure: Lost task 1.0 in stage 4.0 (TID 13) (192.168.1.249 executor driver): java.lang.NoClassDefFoundError: Could not initialize class org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper.getOrCreateFormatter(DateTimeFormatterHelper.scala:120)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper.getOrCreateFormatter$(DateTimeFormatterHelper.scala:116)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.getOrCreateFormatter(TimestampFormatter.scala:153)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.formatter$lzycompute(TimestampFormatter.scala:162)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.formatter(TimestampFormatter.scala:161)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.validatePatternString(TimestampFormatter.scala:254)\n",
      "\tat org.apache.spark.sql.catalyst.util.TimestampFormatter$.getFormatter(TimestampFormatter.scala:523)\n",
      "\tat org.apache.spark.sql.catalyst.util.TimestampFormatter$.apply(TimestampFormatter.scala:557)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityGenerator.<init>(UnivocityGenerator.scala:51)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:40)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.NoClassDefFoundError: Could not initialize class org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper.getOrCreateFormatter(DateTimeFormatterHelper.scala:120)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper.getOrCreateFormatter$(DateTimeFormatterHelper.scala:116)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.getOrCreateFormatter(TimestampFormatter.scala:153)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.formatter$lzycompute(TimestampFormatter.scala:162)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.formatter(TimestampFormatter.scala:161)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.validatePatternString(TimestampFormatter.scala:254)\n",
      "\tat org.apache.spark.sql.catalyst.util.TimestampFormatter$.getFormatter(TimestampFormatter.scala:523)\n",
      "\tat org.apache.spark.sql.catalyst.util.TimestampFormatter$.apply(TimestampFormatter.scala:557)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityGenerator.<init>(UnivocityGenerator.scala:51)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:40)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "25/02/01 10:21:12 WARN TaskSetManager: Lost task 2.0 in stage 4.0 (TID 14) (192.168.1.249 executor driver): java.lang.InternalError: java.io.FileNotFoundException: /home/yuri/Documents/Data Engineering/PySpark/JDK 8/jre/lib/ext/cldrdata.jar\n",
      "\tat sun.misc.URLClassPath$JarLoader.getResource(URLClassPath.java:1066)\n",
      "\tat sun.misc.URLClassPath.getResource(URLClassPath.java:250)\n",
      "\tat java.net.URLClassLoader$1.run(URLClassLoader.java:366)\n",
      "\tat java.net.URLClassLoader$1.run(URLClassLoader.java:363)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat java.net.URLClassLoader.findClass(URLClassLoader.java:362)\n",
      "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n",
      "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n",
      "\tat java.util.ResourceBundle$RBClassLoader.loadClass(ResourceBundle.java:512)\n",
      "\tat java.util.ResourceBundle$Control.newBundle(ResourceBundle.java:2657)\n",
      "\tat java.util.ResourceBundle.loadBundle(ResourceBundle.java:1518)\n",
      "\tat java.util.ResourceBundle.findBundle(ResourceBundle.java:1482)\n",
      "\tat java.util.ResourceBundle.getBundleImpl(ResourceBundle.java:1370)\n",
      "\tat java.util.ResourceBundle.getBundle(ResourceBundle.java:899)\n",
      "\tat sun.util.resources.LocaleData$2.run(LocaleData.java:178)\n",
      "\tat sun.util.resources.LocaleData$2.run(LocaleData.java:173)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat sun.util.resources.LocaleData.getSupplementary(LocaleData.java:173)\n",
      "\tat sun.util.resources.LocaleData.setSupplementary(LocaleData.java:143)\n",
      "\tat sun.util.resources.LocaleData.setSupplementary(LocaleData.java:133)\n",
      "\tat sun.util.locale.provider.LocaleResources.getJavaTimeFormatData(LocaleResources.java:457)\n",
      "\tat sun.util.locale.provider.LocaleResources.getJavaTimeNames(LocaleResources.java:344)\n",
      "\tat sun.util.locale.provider.CalendarNameProviderImpl.getDisplayNamesImpl(CalendarNameProviderImpl.java:157)\n",
      "\tat sun.util.locale.provider.CalendarNameProviderImpl.getJavaTimeDisplayNames(CalendarNameProviderImpl.java:147)\n",
      "\tat sun.util.locale.provider.CalendarDataUtility$CalendarFieldValueNamesMapGetter.getObject(CalendarDataUtility.java:175)\n",
      "\tat sun.util.locale.provider.CalendarDataUtility$CalendarFieldValueNamesMapGetter.getObject(CalendarDataUtility.java:154)\n",
      "\tat sun.util.locale.provider.LocaleServiceProviderPool.getLocalizedObjectImpl(LocaleServiceProviderPool.java:281)\n",
      "\tat sun.util.locale.provider.LocaleServiceProviderPool.getLocalizedObject(LocaleServiceProviderPool.java:265)\n",
      "\tat sun.util.locale.provider.CalendarDataUtility.retrieveJavaTimeFieldValueNames(CalendarDataUtility.java:96)\n",
      "\tat java.time.format.DateTimeTextProvider.createStore(DateTimeTextProvider.java:352)\n",
      "\tat java.time.format.DateTimeTextProvider.findStore(DateTimeTextProvider.java:312)\n",
      "\tat java.time.format.DateTimeTextProvider.getText(DateTimeTextProvider.java:141)\n",
      "\tat java.time.format.DateTimeFormatterBuilder$TextPrinterParser.format(DateTimeFormatterBuilder.java:3110)\n",
      "\tat java.time.format.DateTimeFormatterBuilder$CompositePrinterParser.format(DateTimeFormatterBuilder.java:2190)\n",
      "\tat java.time.format.DateTimeFormatter.formatTo(DateTimeFormatter.java:1746)\n",
      "\tat java.time.format.DateTimeFormatter.format(DateTimeFormatter.java:1720)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.<init>(DateTimeFormatterHelper.scala:272)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.<clinit>(DateTimeFormatterHelper.scala)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper.getOrCreateFormatter(DateTimeFormatterHelper.scala:120)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper.getOrCreateFormatter$(DateTimeFormatterHelper.scala:116)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.getOrCreateFormatter(TimestampFormatter.scala:153)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.formatter$lzycompute(TimestampFormatter.scala:162)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.formatter(TimestampFormatter.scala:161)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.validatePatternString(TimestampFormatter.scala:254)\n",
      "\tat org.apache.spark.sql.catalyst.util.TimestampFormatter$.getFormatter(TimestampFormatter.scala:523)\n",
      "\tat org.apache.spark.sql.catalyst.util.TimestampFormatter$.apply(TimestampFormatter.scala:557)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityGenerator.<init>(UnivocityGenerator.scala:51)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:40)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.io.FileNotFoundException: /home/yuri/Documents/Data Engineering/PySpark/JDK 8/jre/lib/ext/cldrdata.jar\n",
      "\tat sun.misc.URLClassPath$JarLoader.getJarFile(URLClassPath.java:939)\n",
      "\tat sun.misc.URLClassPath$JarLoader.access$800(URLClassPath.java:802)\n",
      "\tat sun.misc.URLClassPath$JarLoader$1.run(URLClassPath.java:887)\n",
      "\tat sun.misc.URLClassPath$JarLoader$1.run(URLClassPath.java:880)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat sun.misc.URLClassPath$JarLoader.ensureOpen(URLClassPath.java:879)\n",
      "\tat sun.misc.URLClassPath$JarLoader.getResource(URLClassPath.java:1064)\n",
      "\t... 68 more\n",
      "\n",
      "25/02/01 10:21:15 ERROR Configuration: error parsing conf core-default.xml\n",
      "java.io.FileNotFoundException: /home/yuri/Documents/Data Engineering/PySpark/.venv/lib/python3.11/site-packages/pyspark/jars/hadoop-client-api-3.3.4.jar (No such file or directory)\n",
      "\tat java.util.zip.ZipFile.open(Native Method)\n",
      "\tat java.util.zip.ZipFile.<init>(ZipFile.java:231)\n",
      "\tat java.util.zip.ZipFile.<init>(ZipFile.java:157)\n",
      "\tat java.util.jar.JarFile.<init>(JarFile.java:169)\n",
      "\tat java.util.jar.JarFile.<init>(JarFile.java:106)\n",
      "\tat sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:93)\n",
      "\tat sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:69)\n",
      "\tat sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)\n",
      "\tat sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:122)\n",
      "\tat sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:152)\n",
      "\tat org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)\n",
      "\tat org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)\n",
      "\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)\n",
      "\tat org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)\n",
      "\tat org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)\n",
      "\tat org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)\n",
      "\tat org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)\n",
      "Exception in thread \"Thread-1\" java.lang.RuntimeException: java.io.FileNotFoundException: /home/yuri/Documents/Data Engineering/PySpark/.venv/lib/python3.11/site-packages/pyspark/jars/hadoop-client-api-3.3.4.jar (No such file or directory)\n",
      "\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3089)\n",
      "\tat org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)\n",
      "\tat org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)\n",
      "\tat org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)\n",
      "\tat org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)\n",
      "Caused by: java.io.FileNotFoundException: /home/yuri/Documents/Data Engineering/PySpark/.venv/lib/python3.11/site-packages/pyspark/jars/hadoop-client-api-3.3.4.jar (No such file or directory)\n",
      "\tat java.util.zip.ZipFile.open(Native Method)\n",
      "\tat java.util.zip.ZipFile.<init>(ZipFile.java:231)\n",
      "\tat java.util.zip.ZipFile.<init>(ZipFile.java:157)\n",
      "\tat java.util.jar.JarFile.<init>(JarFile.java:169)\n",
      "\tat java.util.jar.JarFile.<init>(JarFile.java:106)\n",
      "\tat sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:93)\n",
      "\tat sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:69)\n",
      "\tat sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:99)\n",
      "\tat sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:122)\n",
      "\tat sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:152)\n",
      "\tat org.apache.hadoop.conf.Configuration.parse(Configuration.java:3009)\n",
      "\tat org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3105)\n",
      "\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)\n",
      "\t... 10 more\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yuri/Documents/Data Engineering/PySpark/.venv/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "  File \"/home/yuri/Documents/Data Engineering/PySpark/.venv/lib/python3.11/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yuri/Documents/Data Engineering/PySpark/.venv/lib/python3.11/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yuri/Documents/Data Engineering/PySpark/.venv/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "  File \"/home/yuri/Documents/Data Engineering/PySpark/.venv/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 58144)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/local/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/local/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/local/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/yuri/Documents/Data Engineering/PySpark/.venv/lib/python3.11/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "  File \"/home/yuri/Documents/Data Engineering/PySpark/.venv/lib/python3.11/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "  File \"/home/yuri/Documents/Data Engineering/PySpark/.venv/lib/python3.11/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "  File \"/home/yuri/Documents/Data Engineering/PySpark/.venv/lib/python3.11/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "py4j does not exist in the JVM",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/Data Engineering/PySpark/.venv/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/Data Engineering/PySpark/.venv/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdf_books\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Data Engineering/PySpark/.venv/lib/python3.11/site-packages/pyspark/sql/readwriter.py:1864\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m   1847\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m   1848\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1862\u001b[0m     lineSep\u001b[38;5;241m=\u001b[39mlineSep,\n\u001b[1;32m   1863\u001b[0m )\n\u001b[0;32m-> 1864\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Data Engineering/PySpark/.venv/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/Data Engineering/PySpark/.venv/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:181\u001b[0m, in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n",
      "File \u001b[0;32m~/Documents/Data Engineering/PySpark/.venv/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:173\u001b[0m, in \u001b[0;36mconvert_exception\u001b[0;34m(e)\u001b[0m\n",
      "File \u001b[0;32m~/Documents/Data Engineering/PySpark/.venv/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:65\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, desc, stackTrace, cause, origin)\u001b[0m\n",
      "File \u001b[0;32m~/Documents/Data Engineering/PySpark/.venv/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:143\u001b[0m, in \u001b[0;36mconvert_exception\u001b[0;34m(e)\u001b[0m\n",
      "File \u001b[0;32m~/Documents/Data Engineering/PySpark/.venv/lib/python3.11/site-packages/py4j/java_gateway.py:464\u001b[0m, in \u001b[0;36mis_instance_of\u001b[0;34m(gateway, java_object, java_class)\u001b[0m\n",
      "File \u001b[0;32m~/Documents/Data Engineering/PySpark/.venv/lib/python3.11/site-packages/py4j/java_gateway.py:1725\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n",
      "\u001b[0;31mPy4JError\u001b[0m: py4j does not exist in the JVM"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yuri/Documents/Data Engineering/PySpark/.venv/lib/python3.11/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yuri/Documents/Data Engineering/PySpark/.venv/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "  File \"/home/yuri/Documents/Data Engineering/PySpark/.venv/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "source": [
    "output_path = \"../\"\n",
    "df_books.write.csv(output_path, header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have this downloaded, load it into your Python file and into a PySpark DataFrame. Firstly check the file to ensure that the data is of the right type, and there are no null values. Then I want you to tell me which day had the most lightning strikes recorded. Order the dataframe by descending number of strikes, then save it to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_lightning = spark.read.csv(\"lightening strikes dataset.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+-----------------+\n",
      "|      date|number_of_strikes|center_point_geom|\n",
      "+----------+-----------------+-----------------+\n",
      "|2018-01-03|              194|    POINT(-75 27)|\n",
      "|2018-01-03|               41|  POINT(-78.4 29)|\n",
      "|2018-01-03|               33|  POINT(-73.9 27)|\n",
      "|2018-01-03|               38|  POINT(-73.8 27)|\n",
      "|2018-01-03|               92|    POINT(-79 28)|\n",
      "+----------+-----------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_lightning.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bydays = df_lightning.groupBy(\"date\").agg(sum(\"number_of_strikes\").alias(\"total_count\")).orderBy(col(\"total_count\").desc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "output_path = \"csv_out\"\n",
    "df_bydays.write.csv(output_path, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 76:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+\n",
      "|center_point_geom|avg_strikes|\n",
      "+-----------------+-----------+\n",
      "|POINT(-95.3 29.4)|      17.52|\n",
      "|POINT(-92.6 24.9)|       6.65|\n",
      "|POINT(-92.2 26.1)|       8.64|\n",
      "|POINT(-91.3 28.1)|      11.44|\n",
      "|POINT(-93.9 28.1)|       12.3|\n",
      "|POINT(-87.4 25.2)|       8.94|\n",
      "|POINT(-86.9 26.4)|      11.91|\n",
      "|POINT(-76.3 26.7)|      14.53|\n",
      "|  POINT(-85.5 26)|      10.03|\n",
      "|POINT(-76.1 25.7)|       11.0|\n",
      "|POINT(-99.9 33.6)|      26.79|\n",
      "|  POINT(-77 26.5)|       9.83|\n",
      "|POINT(-79.6 28.3)|      16.95|\n",
      "|POINT(-78.7 31.9)|      16.14|\n",
      "|POINT(-73.5 29.2)|       5.57|\n",
      "|POINT(-96.5 27.7)|      19.76|\n",
      "|POINT(-95.9 33.7)|      27.16|\n",
      "|POINT(-95.1 34.3)|       34.9|\n",
      "|POINT(-94.7 35.2)|      39.15|\n",
      "|  POINT(-92.8 31)|      25.78|\n",
      "+-----------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_lightning.groupBy(\"center_point_geom\").agg(round(avg(\"number_of_strikes\"), 2).alias(\"avg_strikes\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pandas_df = df_books.toPandas()\n",
    "pandas_df.to_csv(\"book.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
